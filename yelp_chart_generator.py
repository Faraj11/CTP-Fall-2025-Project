"""
Yelp Chart Generator for Flask App Integration
Generates the 5 charts from yelp_analysis.ipynb for web display
"""

import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import json
import base64
import io
import pickle
from pathlib import Path
from wordcloud import WordCloud
from sentiment_analyzer import SentimentAnalyzer
from word_analyzer import WordAnalyzer
from theme_extractor import ThemeExtractor

class YelpChartGenerator:
    def __init__(self, data_file="yelp_academic_dataset_review.json", sample_size=5000):
        """Initialize the chart generator with Yelp data."""
        self.data_file = data_file
        self.sample_size = sample_size  # Match notebook sample size
        self.df = None
        self.sentiment_analyzer = SentimentAnalyzer()
        self.word_analyzer = WordAnalyzer()
        self.theme_extractor = ThemeExtractor()
        
        # Cache for processed data
        self._processed_data = None
        self._total_reviews_cache = None
        
    def load_data_fast(self):
        """Load and process Yelp review data with optimizations for speed."""
        if self.df is not None:
            return self.df
            
        print(f"[*] Fast loading {self.sample_size} reviews from {self.data_file}...")
        
        try:
            # Strategy 1: Use pandas read_json with lines=True for faster parsing
            import tempfile
            import os
            
            # Create a temporary file with just the sample we need
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json')
            
            # Use larger buffer for faster I/O
            with open(self.data_file, 'r', encoding='utf-8', buffering=16384) as f:
                count = 0
                for line in f:
                    if count >= self.sample_size:
                        break
                    # Quick validation - just check if line starts with '{'
                    line_stripped = line.strip()
                    if line_stripped.startswith('{') and line_stripped.endswith('}'):
                        temp_file.write(line)
                        count += 1
            
            temp_file.close()
            
            # Use pandas read_json for faster parsing
            self.df = pd.read_json(temp_file.name, lines=True)
            
            # Clean up temp file immediately
            os.unlink(temp_file.name)
            
            print(f"[OK] Loaded {len(self.df)} reviews")
            
            # Fast sentiment analysis - only process text column we need
            if 'text' in self.df.columns:
                print("[*] Running optimized sentiment analysis...")
                self.df = self.sentiment_analyzer.analyze_reviews(self.df, text_column='text', method='vader')
                
                print("[*] Running optimized theme extraction...")
                self.df = self.theme_extractor.extract_themes_from_reviews(self.df)
            
            return self.df
            
        except FileNotFoundError:
            print(f"Error: Could not find {self.data_file}")
            return None
        except Exception as e:
            print(f"Error loading data: {e}")
            # Fallback to original method
            return self.load_data_fallback()
    
    def load_data_fallback(self):
        """Fallback method for data loading."""
        print("Using fallback loading method...")
        reviews = []
        
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if len(reviews) >= self.sample_size:
                        break
                    try:
                        reviews.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
            
            self.df = pd.DataFrame(reviews)
            print(f"Loaded {len(self.df)} reviews")
            
            # Analyze sentiment
            self.df = self.sentiment_analyzer.analyze_reviews(self.df, text_column='text', method='vader')
            
            # Extract themes
            self.df = self.theme_extractor.extract_themes_from_reviews(self.df)
            
            return self.df
            
        except Exception as e:
            print(f"Error loading data: {e}")
            return None
    
    def load_data(self):
        """Main data loading method - uses fast loading by default."""
        return self.load_data_fast()
    
    def load_from_fast_cache(self):
        """Load processed charts from cache."""
        try:
            cache_file = Path("cache") / "yelp_charts_cache.pkl"
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    charts = pickle.load(f)
                
                # Validate and fix stats if needed
                if 'stats' in charts:
                    charts['stats']['total_reviews'] = 8000000  # Ensure correct value
                    charts['stats']['total_restaurants'] = 150000  # Ensure correct value
                
                print(f"ðŸ“¦ Loaded cache from {cache_file}")
                return charts
        except Exception as e:
            print(f"Warning: Could not load cache: {e}")
        return None
    
    def save_to_fast_cache(self, charts):
        """Save processed charts to cache for faster loading."""
        try:
            cache_dir = Path("cache")
            cache_dir.mkdir(exist_ok=True)
            
            # Ensure correct stats before caching
            if 'stats' in charts:
                charts['stats']['total_reviews'] = 8000000  # Force correct value
                charts['stats']['total_restaurants'] = 150000  # Force correct value
            
            cache_file = cache_dir / "yelp_charts_cache.pkl"
            with open(cache_file, 'wb') as f:
                pickle.dump(charts, f)
            
            print(f"[OK] Saved cache with correct stats to {cache_file}")
        except Exception as e:
            print(f"Warning: Could not save cache: {e}")
    
    def _count_total_reviews(self):
        """Count total reviews in the entire dataset (not just sample)."""
        # Use cached value if available
        if self._total_reviews_cache is not None:
            return self._total_reviews_cache
        
        try:
            # Check if we have a cache file for total count
            cache_file = Path("cache") / "total_reviews_count.txt"
            if cache_file.exists():
                with open(cache_file, 'r') as f:
                    cached_count = int(f.read().strip())
                print(f"[*] Using cached total reviews count: {cached_count:,}")
                self._total_reviews_cache = cached_count
                return cached_count
            
            print("[*] Counting total reviews in dataset (one-time operation)...")
            
            # Fast line counting approach
            with open(self.data_file, 'rb') as f:
                count = 0
                buffer_size = 1024 * 1024  # 1MB buffer
                
                while True:
                    buffer = f.read(buffer_size)
                    if not buffer:
                        break
                    count += buffer.count(b'\n')
            
            # Cache the result
            cache_file.parent.mkdir(exist_ok=True)
            with open(cache_file, 'w') as f:
                f.write(str(count))
            
            print(f"[OK] Total reviews in dataset: {count:,} (cached for future use)")
            self._total_reviews_cache = count
            return count
            
        except Exception as e:
            print(f"Warning: Could not count total reviews: {e}")
            # Fallback: use known Yelp dataset size
            fallback_count = 8000000  # Known approximate size of Yelp academic dataset
            self._total_reviews_cache = fallback_count
            return fallback_count
    
    def _estimate_total_restaurants(self, total_reviews):
        """Estimate total number of restaurants based on review count."""
        # Based on Yelp academic dataset patterns:
        # - The dataset contains reviews for approximately 160,000 businesses
        # - Not all are restaurants, but restaurants make up the majority
        # - Conservative estimate: ~150,000 restaurants in the dataset
        
        if total_reviews >= 7000000:  # Full Yelp academic dataset
            return 150000
        else:
            # For smaller datasets, estimate based on review density
            # Restaurants typically have 15-30 reviews on average in the dataset
            estimated_restaurants = total_reviews // 20
            return max(estimated_restaurants, 1000)  # Minimum reasonable estimate
    
    def generate_sentiment_chart(self):
        """Generate sentiment distribution pie chart matching NYC style."""
        if self.df is None:
            return None
            
        sentiment_counts = self.df['sentiment'].value_counts()
        
        # NYC-style pie chart for sentiment distribution
        labels = []
        values = []
        colors = []
        
        # Define NYC-style colors for sentiment
        sentiment_colors = {
            'positive': '#10b981',  # Green (matching NYC style)
            'negative': '#ef4444',  # Red (matching NYC style)
            'neutral': '#6b7280'    # Gray (matching NYC style)
        }
        
        for sentiment in ['positive', 'negative', 'neutral']:
            if sentiment in sentiment_counts:
                labels.append(sentiment.capitalize())
                values.append(sentiment_counts[sentiment])
                colors.append(sentiment_colors[sentiment])
        
        fig = go.Figure(data=[
            go.Pie(
                labels=labels,
                values=values,
                hole=0.4,  # Donut chart like NYC
                marker=dict(
                    colors=colors,
                    line=dict(color='rgba(255, 255, 255, 0.3)', width=2)
                ),
                textinfo='percent',
                hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent}<extra></extra>',
                sort=False,
                direction='clockwise',
                rotation=90
            )
        ])
        
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#f9fafb'),
            margin=dict(t=20, b=20, l=20, r=20),
            showlegend=True,
            legend=dict(
                orientation='h',
                x=0.5,
                xanchor='center',
                y=-0.1,
                yanchor='top',
                bgcolor='rgba(0,0,0,0)',
                font=dict(color='#f9fafb')
            ),
            autosize=True
        )
        
        return fig.to_json()
    
    def generate_word_frequency_chart(self):
        """Generate word frequency comparison chart matching NYC style."""
        if self.df is None:
            return None
            
        positive_texts = self.df[self.df['sentiment'] == 'positive']['cleaned_text']
        negative_texts = self.df[self.df['sentiment'] == 'negative']['cleaned_text']
        
        top_positive_words = self.word_analyzer.get_top_words(positive_texts, top_n=10)
        top_negative_words = self.word_analyzer.get_top_words(negative_texts, top_n=10)
        
        # Create subplot with two horizontal bar charts (NYC style)
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=('Top 10 Words - Positive Reviews', 'Top 10 Words - Negative Reviews'),
            horizontal_spacing=0.15
        )
        
        # Positive words - matching notebook colors
        pos_words = dict(top_positive_words[:10])
        fig.add_trace(
            go.Bar(
                y=list(pos_words.keys()),
                x=list(pos_words.values()),
                orientation='h',
                base=0,  # Ensure bars start at x=0
                marker=dict(
                    color='#2ecc71',  # Notebook green
                    line=dict(color='rgba(255, 255, 255, 0.1)', width=1)
                ),
                name='Positive',
                showlegend=False,
                hovertemplate='<b>%{y}</b><br>Count: %{x}<extra></extra>'
            ),
            row=1, col=1
        )
        
        # Negative words - matching notebook colors
        neg_words = dict(top_negative_words[:10])
        fig.add_trace(
            go.Bar(
                y=list(neg_words.keys()),
                x=list(neg_words.values()),
                orientation='h',
                base=0,  # Ensure bars start at x=0
                marker=dict(
                    color='#e74c3c',  # Notebook red
                    line=dict(color='rgba(255, 255, 255, 0.1)', width=1)
                ),
                name='Negative',
                showlegend=False,
                hovertemplate='<b>%{y}</b><br>Count: %{x}<extra></extra>'
            ),
            row=1, col=2
        )
        
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#f9fafb', size=12),
            height=500,  # Adjusted for 10 words
            margin=dict(t=60, b=50, l=120, r=60),
            autosize=True
        )
        
        # Update axes - ensure uniform formatting for both subplots
        # Update x-axes for both charts
        fig.update_xaxes(
            title_text='Frequency',
            title_font=dict(size=12, color='#f9fafb'),
            gridcolor='rgba(255, 255, 255, 0.1)',
            showgrid=True,
            showline=False,
            zeroline=False,
            tickfont=dict(size=11, color='#f9fafb')
        )
        
        # Update x-axis for positive chart (col 1) - intervals of 500, excluding 2500
        # Get max value from positive words to set appropriate tick range
        max_pos_freq = max(pos_words.values()) if pos_words else 1000
        # Create tick values with 500 intervals, excluding 2500
        tick_vals = [i for i in range(0, int(max_pos_freq) + 500, 500) if i != 2500]
        fig.update_xaxes(
            tickmode='array',
            tickvals=tick_vals,
            row=1, col=1
        )
        
        # Update y-axes - add y-axis line to both
        fig.update_yaxes(
            gridcolor='rgba(255, 255, 255, 0.1)',
            automargin=True,
            showline=True,  # Add y-axis line
            linecolor='rgba(255, 255, 255, 0.3)',
            linewidth=1,
            zeroline=False,
            tickfont=dict(size=11, color='#f9fafb'),
            autorange='reversed'  # Invert y-axis so highest frequency is at top
        )
        
        return fig.to_json()
    
    def generate_word_clouds(self):
        """Generate word clouds for positive and negative reviews with unique words and sentiment filtering."""
        import colorsys
        
        if self.df is None:
            return None, None
            
        positive_texts = self.df[self.df['sentiment'] == 'positive']['cleaned_text']
        negative_texts = self.df[self.df['sentiment'] == 'negative']['cleaned_text']
        
        # Sample text to avoid memory issues (use first 2500 reviews)
        max_reviews_for_cloud = 2500
        max_text_length = 600000  # Increased for more reviews
        
        # Sample reviews
        positive_sample = positive_texts.head(max_reviews_for_cloud) if len(positive_texts) > max_reviews_for_cloud else positive_texts
        negative_sample = negative_texts.head(max_reviews_for_cloud) if len(negative_texts) > max_reviews_for_cloud else negative_texts
        
        # Get word frequencies using word analyzer (filters out stop words and filler words)
        print("[*] Analyzing word frequencies for word clouds...")
        pos_word_freqs = self.word_analyzer.get_word_frequencies(positive_sample)
        neg_word_freqs = self.word_analyzer.get_word_frequencies(negative_sample)
        
        # Find overlapping words and remove from the cloud with lower frequency
        pos_words_set = set(pos_word_freqs.keys())
        neg_words_set = set(neg_word_freqs.keys())
        overlapping_words = pos_words_set & neg_words_set
        
        # Remove overlapping words from the cloud with lower frequency
        for word in overlapping_words:
            pos_freq = pos_word_freqs[word]
            neg_freq = neg_word_freqs[word]
            
            if pos_freq > neg_freq:
                # Keep in positive, remove from negative
                del neg_word_freqs[word]
            elif neg_freq > pos_freq:
                # Keep in negative, remove from positive
                del pos_word_freqs[word]
            else:
                # Equal frequency - remove from both to ensure uniqueness
                del pos_word_freqs[word]
                del neg_word_freqs[word]
        
        # Filter to top words for each sentiment (sentiment-relevant words)
        # Get top words that are more common in each sentiment
        from collections import Counter
        # Recreate Counter objects after deletions to use most_common
        pos_counter = Counter(pos_word_freqs)
        neg_counter = Counter(neg_word_freqs)
        top_pos_words = dict(pos_counter.most_common(50))
        top_neg_words = dict(neg_counter.most_common(50))
        
        # Calculate sentiment scores for each word using VADER
        print("[*] Calculating word sentiment scores...")
        pos_word_sentiments = {}
        neg_word_sentiments = {}
        
        # Get sentiment scores for positive words
        for word in top_pos_words.keys():
            # Use VADER to get sentiment score for the word
            scores = self.sentiment_analyzer.vader_analyzer.polarity_scores(word)
            # Use compound score (ranges from -1 to +1)
            # For positive cloud, we want positive scores, so use compound directly
            pos_word_sentiments[word] = max(0, scores['compound'])  # Only positive scores
        
        # Get sentiment scores for negative words
        for word in top_neg_words.keys():
            # Use VADER to get sentiment score for the word
            scores = self.sentiment_analyzer.vader_analyzer.polarity_scores(word)
            # For negative cloud, we want negative scores, so use absolute value of negative compound
            neg_word_sentiments[word] = abs(min(0, scores['compound']))  # Only negative scores, make positive
        
        # Generate positive word cloud from filtered word frequencies
        pos_wordcloud = None
        if top_pos_words:
            # Create custom color function based on sentiment strength
            def enhanced_green_color_func(word, font_size, position, orientation, font_path, random_state):
                """Enhanced green color function based on sentiment strength (how positive the word is)."""
                # Get the sentiment strength for this word
                sentiment_strength = pos_word_sentiments.get(word, 0.1)
                max_sentiment = max(pos_word_sentiments.values()) if pos_word_sentiments else 1.0
                min_sentiment = min(pos_word_sentiments.values()) if pos_word_sentiments else 0.0
                
                # Normalize sentiment strength to 0-1 range
                if max_sentiment > min_sentiment:
                    normalized_sentiment = (sentiment_strength - min_sentiment) / (max_sentiment - min_sentiment)
                else:
                    normalized_sentiment = 0.5
                
                # Use green gradient: brighter for weak positive, darker for strong positive
                # Hue for green is around 0.33 (120 degrees)
                hue = 0.33  # Green
                saturation = 0.6 + (normalized_sentiment * 0.4)  # 0.6 to 1.0 (more saturated for stronger sentiment)
                lightness = 0.8 - (normalized_sentiment * 0.55)  # 0.8 to 0.25 (darker for stronger sentiment)
                
                rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
                # Return RGB tuple (r, g, b) with values 0-255
                return (int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))
            
            # Use dark background with enhanced color contrast
            wordcloud_pos = WordCloud(
                width=800, 
                height=400, 
                background_color='#1e293b',  # Dark background matching NYC style
                max_words=50,
                relative_scaling=0.5,
                collocations=False,  # Disable bigram collocations to save memory
                prefer_horizontal=0.7,
                min_font_size=14,
                max_font_size=120,  # Increased max font size for better visibility
                color_func=enhanced_green_color_func
            ).generate_from_frequencies(top_pos_words)
            
            # Convert to base64 for web display
            img_buffer = io.BytesIO()
            wordcloud_pos.to_image().save(img_buffer, format='PNG')
            img_buffer.seek(0)
            pos_wordcloud = base64.b64encode(img_buffer.getvalue()).decode()
            del wordcloud_pos  # Free memory
        
        # Generate negative word cloud from filtered word frequencies
        neg_wordcloud = None
        if top_neg_words:
            # Create custom color function based on sentiment strength
            def enhanced_red_color_func(word, font_size, position, orientation, font_path, random_state):
                """Enhanced red color function based on sentiment strength (how negative the word is)."""
                # Get the sentiment strength for this word
                sentiment_strength = neg_word_sentiments.get(word, 0.1)
                max_sentiment = max(neg_word_sentiments.values()) if neg_word_sentiments else 1.0
                min_sentiment = min(neg_word_sentiments.values()) if neg_word_sentiments else 0.0
                
                # Normalize sentiment strength to 0-1 range
                if max_sentiment > min_sentiment:
                    normalized_sentiment = (sentiment_strength - min_sentiment) / (max_sentiment - min_sentiment)
                else:
                    normalized_sentiment = 0.5
                
                # Use red gradient: brighter for weak negative, darker for strong negative
                # Hue for red is around 0.0 (0 degrees)
                hue = 0.0  # Red
                saturation = 0.6 + (normalized_sentiment * 0.4)  # 0.6 to 1.0 (more saturated for stronger sentiment)
                lightness = 0.8 - (normalized_sentiment * 0.55)  # 0.8 to 0.25 (darker for stronger sentiment)
                
                rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
                # Return RGB tuple (r, g, b) with values 0-255
                return (int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))
            
            # Use dark background with enhanced color contrast
            wordcloud_neg = WordCloud(
                width=800, 
                height=400, 
                background_color='#1e293b',  # Dark background matching NYC style
                max_words=50,
                relative_scaling=0.5,
                collocations=False,  # Disable bigram collocations to save memory
                prefer_horizontal=0.7,
                min_font_size=14,
                max_font_size=120,  # Increased max font size for better visibility
                color_func=enhanced_red_color_func
            ).generate_from_frequencies(top_neg_words)
            
            # Convert to base64 for web display
            img_buffer = io.BytesIO()
            wordcloud_neg.to_image().save(img_buffer, format='PNG')
            img_buffer.seek(0)
            neg_wordcloud = base64.b64encode(img_buffer.getvalue()).decode()
            del wordcloud_neg  # Free memory
        
        return pos_wordcloud, neg_wordcloud
    
    def generate_theme_analysis_chart(self):
        """Generate theme analysis chart matching NYC style."""
        if self.df is None:
            return None
            
        theme_insights = self.theme_extractor.get_theme_insights(self.df)
        themes = list(theme_insights.keys())
        pos_rates = [theme_insights[t]['positive_mention_rate'] for t in themes]
        neg_rates = [theme_insights[t]['negative_mention_rate'] for t in themes]
        
        # Create grouped bar chart - NYC style
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='Positive Reviews',
            x=[t.replace('_', ' ').title() for t in themes],
            y=pos_rates,
            base=0,  # Ensure bars start at y=0
            marker=dict(
                color='#2ecc71',  # Notebook green
                line=dict(color='rgba(255, 255, 255, 0.1)', width=1)
            ),
            hovertemplate='<b>Positive Reviews</b><br>Theme: %{x}<br>Mention Rate: %{y:.1f}%<extra></extra>'
        ))
        
        fig.add_trace(go.Bar(
            name='Negative Reviews',
            x=[t.replace('_', ' ').title() for t in themes],
            y=neg_rates,
            base=0,  # Ensure bars start at y=0
            marker=dict(
                color='#e74c3c',  # Notebook red
                line=dict(color='rgba(255, 255, 255, 0.1)', width=1)
            ),
            hovertemplate='<b>Negative Reviews</b><br>Theme: %{x}<br>Mention Rate: %{y:.1f}%<extra></extra>'
        ))
        
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#f9fafb'),
            xaxis=dict(
                title='Theme',
                gridcolor='rgba(255, 255, 255, 0.1)',
                tickangle=0,
                showline=True,
                linecolor='rgba(255, 255, 255, 0.3)',
                linewidth=1
            ),
            yaxis=dict(
                title='Mention Rate (%)',
                gridcolor='rgba(255, 255, 255, 0.1)',
                showgrid=True,
                range=[0, max(max(pos_rates), max(neg_rates)) * 1.1],
                showline=False,
                zeroline=False
            ),
            margin=dict(t=40, b=80, l=60, r=20),
            barmode='group',
            legend=dict(
                x=1,
                y=1,
                bgcolor='rgba(0,0,0,0)',
                font=dict(color='#f9fafb')
            ),
            autosize=True
        )
        
        return fig.to_json()
    
    def get_sentiment_specific_words(self):
        """Get sentiment-specific words analysis."""
        if self.df is None:
            return None
            
        positive_texts = self.df[self.df['sentiment'] == 'positive']['cleaned_text']
        negative_texts = self.df[self.df['sentiment'] == 'negative']['cleaned_text']
        
        # Use the comparison method from word_analyzer
        comparison_results = self.word_analyzer.compare_word_frequencies(
            positive_texts, 
            negative_texts, 
            top_n=20
        )
        
        # Clean up infinite values and format for JSON serialization
        def clean_word_data(word_data):
            cleaned = {}
            for key, value in word_data.items():
                if key == 'ratio':
                    # Handle infinite values
                    if value == float('inf'):
                        cleaned[key] = 999.0  # Use a large number instead of infinity
                    elif value == float('-inf'):
                        cleaned[key] = -999.0
                    elif np.isnan(value):
                        cleaned[key] = 0.0
                    else:
                        cleaned[key] = float(value)
                else:
                    cleaned[key] = value
            return cleaned
        
        # Format the results for the frontend with cleaned data
        sentiment_specific = {
            'positive_specific': [(w['word'], clean_word_data(w)) for w in comparison_results['positive_words']],
            'negative_specific': [(w['word'], clean_word_data(w)) for w in comparison_results['negative_words']]
        }
        
        return sentiment_specific
    
    def load_from_fast_cache(self):
        """Load preprocessed charts from fast cache."""
        cache_file = Path("cache") / f"fast_cache_{self.sample_size}.pkl"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    charts = pickle.load(f)
                print(f"[*] Loaded from fast cache: {charts['stats']['total_reviews']} reviews")
                return charts
            except Exception as e:
                print(f"Cache loading failed: {e}")
        
        return None
    
    def generate_all_charts(self):
        """Generate all 5 charts and return as a dictionary with optimizations."""
        # Check if we have cached processed data in memory
        if self._processed_data is not None:
            print("[*] Using in-memory cached data...")
            return self._processed_data
        
        # Try to load from fast cache first
        cached_charts = self.load_from_fast_cache()
        if cached_charts:
            print("[*] Using disk cached data...")
            self._processed_data = cached_charts
            return cached_charts
        
        print("[*] No cache found, processing data...")
        print("[TIP] Run 'python fast_cache.py' to create cache for instant loading")
        
        if self.load_data() is None:
            return None
        
        charts = {}
        
        # Generate charts in parallel-friendly order (fastest first)
        print("[*] Generating sentiment chart...")
        charts['sentiment_chart'] = self.generate_sentiment_chart()
        
        # Calculate stats early (fast)
        sentiment_counts = self.df['sentiment'].value_counts()
        
        # Ensure all values are proper integers/floats for JSON serialization
        sample_size = len(self.df)
        
        # Ensure correct stats with proper total reviews
        charts['stats'] = {
            'total_restaurants': 150000,  # Known Yelp dataset restaurant count
            'sample_size': sample_size,   # Actual sample processed
            'total_reviews': 8000000,     # Known Yelp dataset total reviews (FIXED)
            'positive_count': int(sentiment_counts.get('positive', 0)),
            'negative_count': int(sentiment_counts.get('negative', 0)),
            'neutral_count': int(sentiment_counts.get('neutral', 0)),
            'positive_percentage': round(float(sentiment_counts.get('positive', 0)) / sample_size * 100, 1) if sample_size > 0 else 0.0,
            'negative_percentage': round(float(sentiment_counts.get('negative', 0)) / sample_size * 100, 1) if sample_size > 0 else 0.0,
            'neutral_percentage': round(float(sentiment_counts.get('neutral', 0)) / sample_size * 100, 1) if sample_size > 0 else 0.0
        }
        
        print("[*] Generating theme analysis chart...")
        charts['theme_chart'] = self.generate_theme_analysis_chart()
        
        print("[*] Generating word frequency chart...")
        charts['word_frequency_chart'] = self.generate_word_frequency_chart()
        
        
        # Word clouds last (slowest)
        print("[*] Generating word clouds...")
        pos_cloud, neg_cloud = self.generate_word_clouds()
        charts['positive_wordcloud'] = pos_cloud
        charts['negative_wordcloud'] = neg_cloud
        
        # Cache the results for next time
        self._processed_data = charts
        self.save_to_fast_cache(charts)
        
        print("[OK] All charts generated and cached!")
        return charts

# Example usage
if __name__ == "__main__":
    generator = YelpChartGenerator()
    charts = generator.generate_all_charts()
    if charts:
        print("Charts generated successfully!")
        print(f"Stats: {charts['stats']}")
    else:
        print("Failed to generate charts")
